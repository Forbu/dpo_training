{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# we want to use the pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/codespace/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-1cfce6f8a62eee84/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Anthropic/hh-rlhf\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# currently we choose the gpt2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': {'input_ids': tensor([[  198,   198, 20490,    25,  1867,   389,   617,   269,  1046,  2456,\n",
      "           287, 46932,    30,   198,   198, 48902,    25,  3423,   447,   247,\n",
      "            82,   281, 17503,  1351,    13,   198,   198,  8021,    11, 19317,\n",
      "            11,   809, 26679,    11, 18824,    11,  5089,    11,  7510,    11,\n",
      "         21551,    11,   256,  2799,    11,  7510,  2256,    11,  7510, 21454,\n",
      "            11,   629, 10599,   388,    11, 40267,    11, 40107,    11,  5089,\n",
      "           263,    11,  7510,    12, 30041,    11, 10973,    11,   269,  2178,\n",
      "         38811,    11,  5089,    77,  1018,  1136,    11,   475,   400,  2305,\n",
      "            11, 40125,    11, 14509,   562,    11,   269,  3320, 12603,    11,\n",
      "         29836,    11, 43546,    11, 18314,    11, 19311,    11,  6611,    11,\n",
      "           266,   962,    11,   474,  1042,    11, 10973,    12,    82, 19296,\n",
      "            11, 22938,   378,    11,   277,  9460,   313,    11, 24506,    11,\n",
      "           474,  6457,    11,   474,  6457,    12,    75,  7958,    11, 37833,\n",
      "            11, 33526,    11,  1125,   729,    11,   329,  6988,  1352,    11,\n",
      "           781,  2238,  7357,    11,  9583,  1891,    11, 10816,    11, 16949,\n",
      "            11, 32581,   296,   578,    11,  3095,  1136,    11,   285,  1689,\n",
      "           447,   247,    82,  2933,    11,   277,  9460,   313,    11,   583,\n",
      "          1851,    11, 24506,    11,   629,  2178,   363,    11, 21551,    11,\n",
      "           198,   198, 20490,    25,  1867,   338,   534,  4004,   530,    30,\n",
      "           198,   198, 48902,    25,   314,  4398,   470,   772,  1807,   546,\n",
      "           340,    13],\n",
      "        [  198,   198, 20490,    25,  1867,  1611,   286, 26782,   750, 27199,\n",
      "           787,    30,   198,   198, 48902,    25, 27411,   290, 27199,  1422,\n",
      "           447,   247,    83,  2107,   379,   262,   976,   640,    11,   523,\n",
      "           340,   447,   247,    82,  1107,  1327,   284,   910,    13,   383,\n",
      "          1266,  1295,   284,  1064,   503,   644, 26782, 27199,   925,   561,\n",
      "           307,   198,   198, 20490,    25,  3763,   484,   750,   198,   198,\n",
      "         48902,    25,   284,  4724,    11,   290,   326,   561,  2192,  2421,\n",
      "          6041,   286,  3555,   290,   257,  1728,  2033,   286, 13843,    11,\n",
      "           523,   356,   447,   247,   260,   407,  1107,  5597,   284,   466,\n",
      "           326,    13,   198,   198, 20490,    25,   345, 18548,  1100,   198,\n",
      "           198, 48902,    25,   921,   460,  1100,    30, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1002,   345,   547,  1016,   284,  8711,\n",
      "           422,   257, 15607,  3650,    11,   466,   345,   892,   340,   561,\n",
      "           307,  1365,   287,   262,  3329,   393,   379,  1755,    30,   198,\n",
      "           198, 48902,    25,   314,  1107,  3521,   447,   247,    83,   910,\n",
      "            11,   314,   447,   247,    76,   407,  5385,   351, 16568, 15607,\n",
      "          3650,  3709,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1680,   345,  2148,   502,   262,  1363,\n",
      "          2209,   329,  8674,  6047, 21995,    30,   198,   198, 48902,    25,\n",
      "          9022,  8674,   466,   345,  1612,    11,  3446,    30,   220,  1002,\n",
      "           345,  1612,  6047, 21995,    11,   788,   465,  1363,  2209,   318,\n",
      "         13374,  3901,   370,  7299,  3245,  9974,    11, 11733,  2254,    11,\n",
      "          7257,   860,  1433,  3023,    13,   198,   198, 20490,    25,  2141,\n",
      "           423,   257,  3072,  1271,   393,  3053,  2209,   329,   683,   355,\n",
      "           880,    13,   314,   765,   683,   284,   760,   326,   314,   716,\n",
      "          2406,    13,   198,   198, 48902,    25,   679,  1595,   447,   247,\n",
      "            83,   423,   257,  7271,  1695,  3072,  1271,   393,  3053,  2209,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}, 'rejected': {'input_ids': tensor([[  198,   198, 20490,    25,  1867,   389,   617,   269,  1046,  2456,\n",
      "           287, 46932,    30,   198,   198, 48902,    25,  3423,   447,   247,\n",
      "            82,   281, 17503,  1351,    13,   198,   198,  8021,    11, 19317,\n",
      "            11,   809, 26679,    11, 18824,    11,  5089,    11,  7510,    11,\n",
      "         21551,    11,   256,  2799,    11,  7510,  2256,    11,  7510, 21454,\n",
      "            11,   629, 10599,   388,    11, 40267,    11, 40107,    11,  5089,\n",
      "           263,    11,  7510,    12, 30041,    11, 10973,    11,   269,  2178,\n",
      "         38811,    11,  5089,    77,  1018,  1136,    11,   475,   400,  2305,\n",
      "            11, 40125,    11, 14509,   562,    11,   269,  3320, 12603,    11,\n",
      "         29836,    11, 43546,    11, 18314,    11, 19311,    11,  6611,    11,\n",
      "           266,   962,    11,   474,  1042,    11, 10973,    12,    82, 19296,\n",
      "            11, 22938,   378,    11,   277,  9460,   313,    11, 24506,    11,\n",
      "           474,  6457,    11,   474,  6457,    12,    75,  7958,    11, 37833,\n",
      "            11, 33526,    11,  1125,   729,    11,   329,  6988,  1352,    11,\n",
      "           781,  2238,  7357,    11,  9583,  1891,    11, 10816,    11, 16949,\n",
      "            11, 32581,   296,   578,    11,  3095,  1136,    11,   285,  1689,\n",
      "           447,   247,    82,  2933,    11,   277,  9460,   313,    11,   583,\n",
      "          1851,    11, 24506,    11,   629,  2178,   363,    11, 21551,    11,\n",
      "           198,   198, 20490,    25,  1867,   338,   534,  4004,   530,    30,\n",
      "           198,   198, 48902,    25,  2195,    13],\n",
      "        [  198,   198, 20490,    25,  1867,  1611,   286, 26782,   750, 27199,\n",
      "           787,    30,   198,   198, 48902,    25, 27411,   290, 27199,  1422,\n",
      "           447,   247,    83,  2107,   379,   262,   976,   640,    11,   523,\n",
      "           340,   447,   247,    82,  1107,  1327,   284,   910,    13,   383,\n",
      "          1266,  1295,   284,  1064,   503,   644, 26782, 27199,   925,   561,\n",
      "           307,   198,   198, 20490,    25,  3763,   484,   750,   198,   198,\n",
      "         48902,    25,   284,  4724,    11,   290,   326,   561,  2192,  2421,\n",
      "          6041,   286,  3555,   290,   257,  1728,  2033,   286, 13843,    11,\n",
      "           523,   356,   447,   247,   260,   407,  1107,  5597,   284,   466,\n",
      "           326,    13,   198,   198, 20490,    25,   345, 18548,  1100,   198,\n",
      "           198, 48902,    25,   612,   447,   247,    82,   257,  1256,   286,\n",
      "          3404,  5384,   836,   447,   247,    83,   760, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1002,   345,   547,  1016,   284,  8711,\n",
      "           422,   257, 15607,  3650,    11,   466,   345,   892,   340,   561,\n",
      "           307,  1365,   287,   262,  3329,   393,   379,  1755,    30,   198,\n",
      "           198, 48902,    25,   632,   318,   922,   284,  2074,   262,  3580,\n",
      "           287,  1692,  4979,   379,  1755,    11,   290,   635,   262,  2324,\n",
      "          9073,    13,   220,  5265,   640,   561,  2192,   307,  1365,   329,\n",
      "         14928,  2324,  9073,    11,   475,   345,   561,   307,   517,  7424,\n",
      "           284,   262,  3650,  4409,   379,  1755,    13,   220,  1550,   262,\n",
      "           584,  1021,    11,  1201,   340,   318,  3221,  1342,  8179,  1141,\n",
      "           262,  1110,    11,   340,   318,  4577,   284,  3368,   852,  6810,\n",
      "           416,   584,   661,    13,   220,   554,   597,  1339,    11,   340,\n",
      "           318,   845,  1593,   284,  1414,  1969,  3241,   284,   262,  4028,\n",
      "           286,   262,  3650,  4409,   355,   880,   355,   477,   286,   262,\n",
      "          4297,    13,   220,  1119,  1244,   307,  1498,   284,   766,   345,\n",
      "            11,   772,   611,   484,   389,   407,  3264,  6476,   345,    13,\n",
      "           220,   554,  1502,   284,   307,  3338,    11,   340,   318,  1593,\n",
      "           284,  1414,  1969,  3241,   284,  2279,  1016,   319,   287,   262,\n",
      "          3650,    11,   290,   407,   284,   307,   287,  1078,   298,   425,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1680,   345,  2148,   502,   262,  1363,\n",
      "          2209,   329,  8674,  6047, 21995,    30,   198,   198, 48902,    25,\n",
      "          9022,  8674,   466,   345,  1612,    11,  3446,    30,   220,  1002,\n",
      "           345,  1612,  6047, 21995,    11,   788,   465,  1363,  2209,   318,\n",
      "         13374,  3901,   370,  7299,  3245,  9974,    11, 11733,  2254,    11,\n",
      "          7257,   860,  1433,  3023,    13,   198,   198, 20490,    25,  2141,\n",
      "           423,   257,  3072,  1271,   393,  3053,  2209,   329,   683,   355,\n",
      "           880,    13,   314,   765,   683,   284,   760,   326,   314,   716,\n",
      "          2406,    13,   198,   198, 48902,    25,  1002,   345,   765,   284,\n",
      "          2800,  6047, 21995,   416,  3072,   393,  3053,    11,   788,   314,\n",
      "           460,  1037,   345,   351,   326,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "# now we want to transform the dataset pytorch dataloader\n",
    "# we have to create a custom collate function\n",
    "max_length = 512\n",
    "\n",
    "def compute_input_ids(list_sample_tokenized):\n",
    "    max_length_chosen = max([len(sample[\"input_ids\"]) for sample in list_sample_tokenized])\n",
    "    tokenized_pad = [tokenizer.pad(sample, max_length=max_length_chosen, padding=\"max_length\") for sample in list_sample_tokenized]\n",
    "\n",
    "    tokenized_pad_id = [sample[\"input_ids\"] for sample in tokenized_pad]\n",
    "    tokenized_pad_id = torch.tensor(tokenized_pad_id)\n",
    "\n",
    "    chosen_attention_masks = [sample[\"attention_mask\"] for sample in tokenized_pad]\n",
    "    chosen_attention_masks = torch.tensor(chosen_attention_masks)\n",
    "\n",
    "    return tokenized_pad_id, chosen_attention_masks\n",
    "\n",
    "def collate_fn(list_of_samples):\n",
    "    \"\"\"\n",
    "    In this function we define how we want to collate (combine) samples from the dataset\n",
    "    The dataset return a dict with the following keys:\n",
    "        - \"chosen\" : the chosen text\n",
    "        - \"rejected\" : the rejected text\n",
    "    \"\"\"\n",
    "    # we tokenize the chosen and rejected text for every sample\n",
    "    chosen_tokenized = [tokenizer(sample[\"chosen\"], padding=False, truncation=False) for sample in list_of_samples]\n",
    "    rejected_tokenized = [tokenizer(sample[\"rejected\"], padding=False, truncation=False) for sample in list_of_samples]\n",
    "\n",
    "    # we compute the input_ids and attention_masks for the chosen text\n",
    "    chosen_input_ids, chosen_attention_masks = compute_input_ids(chosen_tokenized)\n",
    "\n",
    "    # we compute the input_ids and attention_masks for the rejected text\n",
    "    rejected_input_ids, rejected_attention_masks = compute_input_ids(rejected_tokenized)\n",
    "\n",
    "    # we create a new dict with the input_ids\n",
    "    chosen_input_ids = {\n",
    "        \"input_ids\": chosen_input_ids,\n",
    "        \"attention_mask\": chosen_attention_masks\n",
    "    }\n",
    "\n",
    "    rejected_input_ids = {\n",
    "        \"input_ids\": rejected_input_ids,\n",
    "        \"attention_mask\": rejected_attention_masks\n",
    "    }\n",
    "\n",
    "    # we create a new dict with the tokenized text\n",
    "    tokenized = {\n",
    "        \"chosen\": chosen_input_ids,\n",
    "        \"rejected\": rejected_input_ids\n",
    "    }\n",
    "\n",
    "    # we return the tokenized text\n",
    "    return tokenized\n",
    "\n",
    "# we create the dataloader\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "# we can now iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model\n",
      "torch.Size([4, 202])\n",
      "torch.Size([4, 196])\n"
     ]
    }
   ],
   "source": [
    "# now we can use the training loop from the transformers library\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# we load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# we define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hh-rlhf\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir=\"./hh-rlhf/logs\",\n",
    "    dataloader_num_workers=4,\n",
    "    run_name=\"hh-rlhf\"\n",
    ")\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# we need to define a custom Trainer\n",
    "class DPOTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Class to train the model with DPO (Direct Preference Optimization)\n",
    "    \"\"\"\n",
    "    # we need to define the compute_loss function\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        This function is called by the Trainer during training\n",
    "        This is where we compute the (DPO) loss\n",
    "        \"\"\"\n",
    "        # we get the chosen and rejected text\n",
    "        chosen_text = inputs[\"chosen\"]\n",
    "        rejected_text = inputs[\"rejected\"]\n",
    "\n",
    "        print(\"before model\")\n",
    "\n",
    "        print(chosen_text['input_ids'].shape)\n",
    "        print(rejected_text['input_ids'].shape)\n",
    "\n",
    "        logits_chosen = model(**chosen_text).logits\n",
    "        logits_rejected = model(**rejected_text).logits\n",
    "\n",
    "        print(\"after model\")\n",
    "\n",
    "        # try with logits as ref first\n",
    "\n",
    "        pos_logprob = F.log_softmax(logits_chosen, dim=-1)\n",
    "        neg_logprob = F.log_softmax(logits_rejected, dim=-1)\n",
    "\n",
    "        pos_logprob = torch.gather(pos_logprob, 2, chosen_text.unsqueeze(-1))\n",
    "        neg_logprob = torch.gather(neg_logprob, 2, rejected_text.unsqueeze(-1))\n",
    "\n",
    "        # we need to compute the logprob of the reference examples\n",
    "        pos_logits_ref = logits_chosen.detach()\n",
    "        neg_logits_ref = logits_rejected.detach()\n",
    "\n",
    "        pos_logprob_ref = F.log_softmax(pos_logits_ref, dim=-1)\n",
    "        neg_logprob_ref = F.log_softmax(neg_logits_ref, dim=-1)\n",
    "\n",
    "        pos_logprob_ref = torch.gather(\n",
    "            pos_logprob_ref, 2, chosen_text.unsqueeze(-1)\n",
    "        )\n",
    "        neg_logprob_ref = torch.gather(\n",
    "            neg_logprob_ref, 2, rejected_text.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # compute loss and reward\n",
    "        pi_logratios = pos_logprob.mean() - neg_logprob.mean()\n",
    "        ref_logratios = pos_logprob_ref.mean() - neg_logprob_ref.mean()\n",
    "\n",
    "        print(\"yolo\")\n",
    "        print(pi_logratios.shape)\n",
    "        print(ref_logratios.shape)\n",
    "\n",
    "        loss = -F.logsigmoid(self.beta * (pi_logratios - ref_logratios))\n",
    "\n",
    "        return loss\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# try the compute loss function on batch\n",
    "for batch in dataloader:\n",
    "    with torch.no_grad():\n",
    "        loss = dpo_trainer.compute_loss(model, batch)\n",
    "    print(loss)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
