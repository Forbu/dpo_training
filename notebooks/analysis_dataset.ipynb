{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# we want to use the pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/codespace/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-1cfce6f8a62eee84/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Anthropic/hh-rlhf\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# currently we choose the gpt2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': {'input_ids': tensor([[  198,   198, 20490,    25,  1867,   389,   617,   269,  1046,  2456,\n",
      "           287, 46932,    30,   198,   198, 48902,    25,  3423,   447,   247,\n",
      "            82,   281, 17503,  1351,    13,   198,   198,  8021,    11, 19317,\n",
      "            11,   809, 26679,    11, 18824,    11,  5089,    11,  7510,    11,\n",
      "         21551,    11,   256,  2799,    11,  7510,  2256,    11,  7510, 21454,\n",
      "            11,   629, 10599,   388,    11, 40267,    11, 40107,    11,  5089,\n",
      "           263,    11,  7510,    12, 30041,    11, 10973,    11,   269,  2178,\n",
      "         38811,    11,  5089,    77,  1018,  1136,    11,   475,   400,  2305,\n",
      "            11, 40125,    11, 14509,   562,    11,   269,  3320, 12603,    11,\n",
      "         29836,    11, 43546,    11, 18314,    11, 19311,    11,  6611,    11,\n",
      "           266,   962,    11,   474,  1042,    11, 10973,    12,    82, 19296,\n",
      "            11, 22938,   378,    11,   277,  9460,   313,    11, 24506,    11,\n",
      "           474,  6457,    11,   474,  6457,    12,    75,  7958,    11, 37833,\n",
      "            11, 33526,    11,  1125,   729,    11,   329,  6988,  1352,    11,\n",
      "           781,  2238,  7357,    11,  9583,  1891,    11, 10816,    11, 16949,\n",
      "            11, 32581,   296,   578,    11,  3095,  1136,    11,   285,  1689,\n",
      "           447,   247,    82,  2933,    11,   277,  9460,   313,    11,   583,\n",
      "          1851,    11, 24506,    11,   629,  2178,   363,    11, 21551,    11,\n",
      "           198,   198, 20490,    25,  1867,   338,   534,  4004,   530,    30,\n",
      "           198,   198, 48902,    25,   314,  4398,   470,   772,  1807,   546,\n",
      "           340,    13],\n",
      "        [  198,   198, 20490,    25,  1867,  1611,   286, 26782,   750, 27199,\n",
      "           787,    30,   198,   198, 48902,    25, 27411,   290, 27199,  1422,\n",
      "           447,   247,    83,  2107,   379,   262,   976,   640,    11,   523,\n",
      "           340,   447,   247,    82,  1107,  1327,   284,   910,    13,   383,\n",
      "          1266,  1295,   284,  1064,   503,   644, 26782, 27199,   925,   561,\n",
      "           307,   198,   198, 20490,    25,  3763,   484,   750,   198,   198,\n",
      "         48902,    25,   284,  4724,    11,   290,   326,   561,  2192,  2421,\n",
      "          6041,   286,  3555,   290,   257,  1728,  2033,   286, 13843,    11,\n",
      "           523,   356,   447,   247,   260,   407,  1107,  5597,   284,   466,\n",
      "           326,    13,   198,   198, 20490,    25,   345, 18548,  1100,   198,\n",
      "           198, 48902,    25,   921,   460,  1100,    30, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1002,   345,   547,  1016,   284,  8711,\n",
      "           422,   257, 15607,  3650,    11,   466,   345,   892,   340,   561,\n",
      "           307,  1365,   287,   262,  3329,   393,   379,  1755,    30,   198,\n",
      "           198, 48902,    25,   314,  1107,  3521,   447,   247,    83,   910,\n",
      "            11,   314,   447,   247,    76,   407,  5385,   351, 16568, 15607,\n",
      "          3650,  3709,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1680,   345,  2148,   502,   262,  1363,\n",
      "          2209,   329,  8674,  6047, 21995,    30,   198,   198, 48902,    25,\n",
      "          9022,  8674,   466,   345,  1612,    11,  3446,    30,   220,  1002,\n",
      "           345,  1612,  6047, 21995,    11,   788,   465,  1363,  2209,   318,\n",
      "         13374,  3901,   370,  7299,  3245,  9974,    11, 11733,  2254,    11,\n",
      "          7257,   860,  1433,  3023,    13,   198,   198, 20490,    25,  2141,\n",
      "           423,   257,  3072,  1271,   393,  3053,  2209,   329,   683,   355,\n",
      "           880,    13,   314,   765,   683,   284,   760,   326,   314,   716,\n",
      "          2406,    13,   198,   198, 48902,    25,   679,  1595,   447,   247,\n",
      "            83,   423,   257,  7271,  1695,  3072,  1271,   393,  3053,  2209,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}, 'rejected': {'input_ids': tensor([[  198,   198, 20490,    25,  1867,   389,   617,   269,  1046,  2456,\n",
      "           287, 46932,    30,   198,   198, 48902,    25,  3423,   447,   247,\n",
      "            82,   281, 17503,  1351,    13,   198,   198,  8021,    11, 19317,\n",
      "            11,   809, 26679,    11, 18824,    11,  5089,    11,  7510,    11,\n",
      "         21551,    11,   256,  2799,    11,  7510,  2256,    11,  7510, 21454,\n",
      "            11,   629, 10599,   388,    11, 40267,    11, 40107,    11,  5089,\n",
      "           263,    11,  7510,    12, 30041,    11, 10973,    11,   269,  2178,\n",
      "         38811,    11,  5089,    77,  1018,  1136,    11,   475,   400,  2305,\n",
      "            11, 40125,    11, 14509,   562,    11,   269,  3320, 12603,    11,\n",
      "         29836,    11, 43546,    11, 18314,    11, 19311,    11,  6611,    11,\n",
      "           266,   962,    11,   474,  1042,    11, 10973,    12,    82, 19296,\n",
      "            11, 22938,   378,    11,   277,  9460,   313,    11, 24506,    11,\n",
      "           474,  6457,    11,   474,  6457,    12,    75,  7958,    11, 37833,\n",
      "            11, 33526,    11,  1125,   729,    11,   329,  6988,  1352,    11,\n",
      "           781,  2238,  7357,    11,  9583,  1891,    11, 10816,    11, 16949,\n",
      "            11, 32581,   296,   578,    11,  3095,  1136,    11,   285,  1689,\n",
      "           447,   247,    82,  2933,    11,   277,  9460,   313,    11,   583,\n",
      "          1851,    11, 24506,    11,   629,  2178,   363,    11, 21551,    11,\n",
      "           198,   198, 20490,    25,  1867,   338,   534,  4004,   530,    30,\n",
      "           198,   198, 48902,    25,  2195,    13],\n",
      "        [  198,   198, 20490,    25,  1867,  1611,   286, 26782,   750, 27199,\n",
      "           787,    30,   198,   198, 48902,    25, 27411,   290, 27199,  1422,\n",
      "           447,   247,    83,  2107,   379,   262,   976,   640,    11,   523,\n",
      "           340,   447,   247,    82,  1107,  1327,   284,   910,    13,   383,\n",
      "          1266,  1295,   284,  1064,   503,   644, 26782, 27199,   925,   561,\n",
      "           307,   198,   198, 20490,    25,  3763,   484,   750,   198,   198,\n",
      "         48902,    25,   284,  4724,    11,   290,   326,   561,  2192,  2421,\n",
      "          6041,   286,  3555,   290,   257,  1728,  2033,   286, 13843,    11,\n",
      "           523,   356,   447,   247,   260,   407,  1107,  5597,   284,   466,\n",
      "           326,    13,   198,   198, 20490,    25,   345, 18548,  1100,   198,\n",
      "           198, 48902,    25,   612,   447,   247,    82,   257,  1256,   286,\n",
      "          3404,  5384,   836,   447,   247,    83,   760, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1002,   345,   547,  1016,   284,  8711,\n",
      "           422,   257, 15607,  3650,    11,   466,   345,   892,   340,   561,\n",
      "           307,  1365,   287,   262,  3329,   393,   379,  1755,    30,   198,\n",
      "           198, 48902,    25,   632,   318,   922,   284,  2074,   262,  3580,\n",
      "           287,  1692,  4979,   379,  1755,    11,   290,   635,   262,  2324,\n",
      "          9073,    13,   220,  5265,   640,   561,  2192,   307,  1365,   329,\n",
      "         14928,  2324,  9073,    11,   475,   345,   561,   307,   517,  7424,\n",
      "           284,   262,  3650,  4409,   379,  1755,    13,   220,  1550,   262,\n",
      "           584,  1021,    11,  1201,   340,   318,  3221,  1342,  8179,  1141,\n",
      "           262,  1110,    11,   340,   318,  4577,   284,  3368,   852,  6810,\n",
      "           416,   584,   661,    13,   220,   554,   597,  1339,    11,   340,\n",
      "           318,   845,  1593,   284,  1414,  1969,  3241,   284,   262,  4028,\n",
      "           286,   262,  3650,  4409,   355,   880,   355,   477,   286,   262,\n",
      "          4297,    13,   220,  1119,  1244,   307,  1498,   284,   766,   345,\n",
      "            11,   772,   611,   484,   389,   407,  3264,  6476,   345,    13,\n",
      "           220,   554,  1502,   284,   307,  3338,    11,   340,   318,  1593,\n",
      "           284,  1414,  1969,  3241,   284,  2279,  1016,   319,   287,   262,\n",
      "          3650,    11,   290,   407,   284,   307,   287,  1078,   298,   425,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  198,   198, 20490,    25,  1680,   345,  2148,   502,   262,  1363,\n",
      "          2209,   329,  8674,  6047, 21995,    30,   198,   198, 48902,    25,\n",
      "          9022,  8674,   466,   345,  1612,    11,  3446,    30,   220,  1002,\n",
      "           345,  1612,  6047, 21995,    11,   788,   465,  1363,  2209,   318,\n",
      "         13374,  3901,   370,  7299,  3245,  9974,    11, 11733,  2254,    11,\n",
      "          7257,   860,  1433,  3023,    13,   198,   198, 20490,    25,  2141,\n",
      "           423,   257,  3072,  1271,   393,  3053,  2209,   329,   683,   355,\n",
      "           880,    13,   314,   765,   683,   284,   760,   326,   314,   716,\n",
      "          2406,    13,   198,   198, 48902,    25,  1002,   345,   765,   284,\n",
      "          2800,  6047, 21995,   416,  3072,   393,  3053,    11,   788,   314,\n",
      "           460,  1037,   345,   351,   326,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "# now we want to transform the dataset pytorch dataloader\n",
    "# we have to create a custom collate function\n",
    "max_length = 512\n",
    "\n",
    "def compute_input_ids(list_sample_tokenized):\n",
    "    max_length_chosen = max([len(sample[\"input_ids\"]) for sample in list_sample_tokenized])\n",
    "\n",
    "    # we limit the max_length to 512\n",
    "    max_length_chosen = min(max_length_chosen, max_length)\n",
    "\n",
    "    # we take only the last max_length_chosen tokens of the text\n",
    "    def filter_last_tokens(sample):\n",
    "        return {\"input_ids\" : sample[\"input_ids\"][-max_length_chosen:], \"attention_mask\" : sample[\"attention_mask\"][-max_length_chosen:]}\n",
    "\n",
    "\n",
    "    list_sample_tokenized = [filter_last_tokens(sample) for sample in list_sample_tokenized]\n",
    "\n",
    "    tokenized_pad = [tokenizer.pad(sample, max_length=max_length_chosen, padding=\"max_length\") for sample in list_sample_tokenized]\n",
    "\n",
    "    tokenized_pad_id = [sample[\"input_ids\"] for sample in tokenized_pad]\n",
    "    tokenized_pad_id = torch.tensor(tokenized_pad_id)\n",
    "\n",
    "    chosen_attention_masks = [sample[\"attention_mask\"] for sample in tokenized_pad]\n",
    "    chosen_attention_masks = torch.tensor(chosen_attention_masks)\n",
    "\n",
    "    return tokenized_pad_id, chosen_attention_masks\n",
    "\n",
    "def collate_fn(list_of_samples):\n",
    "    \"\"\"\n",
    "    In this function we define how we want to collate (combine) samples from the dataset\n",
    "    The dataset return a dict with the following keys:\n",
    "        - \"chosen\" : the chosen text\n",
    "        - \"rejected\" : the rejected text\n",
    "    \"\"\"\n",
    "    # we tokenize the chosen and rejected text for every sample\n",
    "    chosen_tokenized = [tokenizer(sample[\"chosen\"], padding=False, truncation=False) for sample in list_of_samples]\n",
    "    rejected_tokenized = [tokenizer(sample[\"rejected\"], padding=False, truncation=False) for sample in list_of_samples]\n",
    "\n",
    "    # we compute the input_ids and attention_masks for the chosen text\n",
    "    chosen_input_ids, chosen_attention_masks = compute_input_ids(chosen_tokenized)\n",
    "\n",
    "    # we compute the input_ids and attention_masks for the rejected text\n",
    "    rejected_input_ids, rejected_attention_masks = compute_input_ids(rejected_tokenized)\n",
    "\n",
    "    # we create a new dict with the input_ids\n",
    "    chosen_input_ids = {\n",
    "        \"input_ids\": chosen_input_ids,\n",
    "        \"attention_mask\": chosen_attention_masks\n",
    "    }\n",
    "\n",
    "    rejected_input_ids = {\n",
    "        \"input_ids\": rejected_input_ids,\n",
    "        \"attention_mask\": rejected_attention_masks\n",
    "    }\n",
    "\n",
    "    # we create a new dict with the tokenized text\n",
    "    tokenized = {\n",
    "        \"chosen\": chosen_input_ids,\n",
    "        \"rejected\": rejected_input_ids\n",
    "    }\n",
    "\n",
    "    # we return the tokenized text\n",
    "    return tokenized\n",
    "\n",
    "# we create the dataloader\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "# we can now iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='80400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/80400 00:04 < 89:29:38, 0.25 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mImpossible d’exécuter le code ; la session a été supprimée. Essayez de redémarrer le noyau."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# now we can use the training loop from the transformers library\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# we load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import copy\n",
    "\n",
    "# we need to define a custom Trainer\n",
    "class DPOTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Class to train the model with DPO (Direct Preference Optimization)\n",
    "    \"\"\"\n",
    "    beta = 0.5\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model_ref = kwargs.pop(\"model_ref\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "        self.model_ref = model_ref\n",
    "        self.model_ref.eval()\n",
    "\n",
    "        self.device_ref = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "        self.model_ref.to(self.device_ref)\n",
    "\n",
    "\n",
    "\n",
    "    # we need to define the compute_loss function\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        This function is called by the Trainer during training\n",
    "        This is where we compute the (DPO) loss\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        This function is called by the Trainer during training\n",
    "        This is where we compute the (DPO) loss\n",
    "        \"\"\"\n",
    "        # we get the chosen and rejected text\n",
    "        chosen_text = inputs[\"chosen\"]\n",
    "        rejected_text = inputs[\"rejected\"]\n",
    "\n",
    "        # labels chosen is just the chosen text shifted by one\n",
    "        labels_chosen = torch.zeros_like(chosen_text[\"input_ids\"]).long()\n",
    "        labels_chosen[:, :-1] = chosen_text[\"input_ids\"][:, 1:]\n",
    "        \n",
    "        # labels rejected is just the rejected text shifted by one\n",
    "        labels_rejected = torch.zeros_like(rejected_text[\"input_ids\"]).long()\n",
    "        labels_rejected[:, :-1] = rejected_text[\"input_ids\"][:, 1:]\n",
    "\n",
    "        logits_chosen = model(**chosen_text).logits\n",
    "        logits_rejected = model(**rejected_text).logits\n",
    "\n",
    "        # try with logits as ref first\n",
    "        pos_logprob = F.log_softmax(logits_chosen, dim=-1)\n",
    "        neg_logprob = F.log_softmax(logits_rejected, dim=-1)\n",
    "\n",
    "\n",
    "        pos_logprob = torch.gather(pos_logprob, 2, labels_chosen.unsqueeze(-1))\n",
    "        neg_logprob = torch.gather(neg_logprob, 2, labels_rejected.unsqueeze(-1))\n",
    "\n",
    "        # we need to compute the logprob of the reference examples\n",
    "        with torch.no_grad():\n",
    "          pos_logits_ref = self.model_ref(**chosen_text).logits\n",
    "          neg_logits_ref = self.model_ref(**rejected_text).logits\n",
    "\n",
    "        pos_logprob_ref = F.log_softmax(pos_logits_ref, dim=-1)\n",
    "        neg_logprob_ref = F.log_softmax(neg_logits_ref, dim=-1)\n",
    "\n",
    "        pos_logprob_ref = torch.gather(\n",
    "            pos_logprob_ref, 2, labels_chosen.unsqueeze(-1)\n",
    "        )\n",
    "        neg_logprob_ref = torch.gather(\n",
    "            neg_logprob_ref, 2, labels_rejected.unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        # TODO COMPUTE MASK FOR PADDING VALUES\n",
    "\n",
    "\n",
    "        # compute loss and reward\n",
    "        pi_logratios = pos_logprob.sum(-1) - neg_logprob.sum(-1)\n",
    "        ref_logratios = pos_logprob_ref.sum(-1) - neg_logprob_ref.sum(-1)\n",
    "        ref_logratios = 0\n",
    "\n",
    "        loss = -F.logsigmoid(self.beta * (pi_logratios - ref_logratios.detach()))\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        collate_fn = self.data_collator\n",
    "        train_dataset = self.train_dataset\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# we define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hh-rlhf\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir=\"./hh-rlhf/logs_4\",\n",
    "    dataloader_num_workers=4,\n",
    "    run_name=\"hh-rlhf_3\",\n",
    "    logging_steps=100,\n",
    "    #bf16=True,\n",
    ")\n",
    "\n",
    "#training_args.set_logging(strategy=\"steps\", steps=100, report_to=\"tensorboard\")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collate_fn,\n",
    "    model_ref=model_ref.eval(),\n",
    ")\n",
    "\n",
    "# we can now train the model\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
